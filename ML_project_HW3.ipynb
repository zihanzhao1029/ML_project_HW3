{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMILK2zQ3tP/p7urS/UrpgW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zihanzhao1029/ML_project_HW3/blob/main/ML_project_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Github link: "
      ],
      "metadata": {
        "id": "O4WpgSVDnU1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Discuss the dataset in general terms and describe why building a predictive model using this data might be practically useful.  Who could benefit from a model like this? Explain."
      ],
      "metadata": {
        "id": "71fsmXtkg0bo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stanford Sentiment Treebank dataset(SST) consists of movie reviews and its corresponding label 0(negative) and 1(postive). Building a predictive model using this dataset can be beneficial for movie production firms, movie critics, and audience rating websites like Rotten Tomatoes to accurately classify movie reviews into positive or negative sentiment categories. In this case, movie prodution firms can update their idea for movies based on audience feedback, and professional movie critics can readily write movie review report.Similarly, the movie rating website can update improve their recommended system based on this predictive model"
      ],
      "metadata": {
        "id": "hJCds59qg8o_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Run at least three prediction models to try to predict the SST sentiment dataset well."
      ],
      "metadata": {
        "id": "w1q9WyNVhHvd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3sT1YpEAfd8"
      },
      "outputs": [],
      "source": [
        "#install aimodelshare library\n",
        "! pip install aimodelshare==0.0.189"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Stanford Sentiment Treebank dataset\n",
        "from aimodelshare import download_data\n",
        "download_data('public.ecr.aws/y2e2a1d6/sst2_competition_data-repository:latest') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCINO90fBR-e",
        "outputId": "661cc768-122d-4743-f17f-813213d15682"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading [=============================================>   ]\n",
            "\n",
            "Data downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up X_train, X_test, and y_train_labels objects\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "X_train=pd.read_csv(\"sst2_competition_data/X_train.csv\", squeeze=True)\n",
        "X_test=pd.read_csv(\"sst2_competition_data/X_test.csv\", squeeze=True)\n",
        "\n",
        "y_train_labels=pd.read_csv(\"sst2_competition_data/y_train_labels.csv\", squeeze=True)\n",
        "\n",
        "# ohe encode Y data\n",
        "y_train = pd.get_dummies(y_train_labels)\n",
        "\n",
        "X_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNQWWPdvBZpe",
        "outputId": "b36cbb1e-ed6b-4c95-e933-3240c37c7a9c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    The Rock is destined to be the 21st Century 's...\n",
              "1    The gorgeously elaborate continuation of `` Th...\n",
              "2    Singer/composer Bryan Adams contributes a slew...\n",
              "3                 Yet the act is still charming here .\n",
              "4    Whether or not you 're enlightened by any of D...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape\n",
        "print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjb68Q07Ii-O",
        "outputId": "c7f83ea2-59b6-467e-ff2b-73ed3884dae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Negative  Positive\n",
            "0            0         1\n",
            "1            0         1\n",
            "2            0         1\n",
            "3            0         1\n",
            "4            0         1\n",
            "...        ...       ...\n",
            "6915         1         0\n",
            "6916         1         0\n",
            "6917         0         1\n",
            "6918         1         0\n",
            "6919         1         0\n",
            "\n",
            "[6920 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing dataset use tf.keras tokenizer\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Build vocabulary from training text data\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# preprocessor tokenizes words and makes sure all documents have the same length\n",
        "def preprocessor(data, maxlen=40, max_words=10000):\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(data)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    X = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "    return X\n",
        "\n",
        "print(preprocessor(X_train).shape)\n",
        "print(preprocessor(X_test).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFdH2QNgBeDZ",
        "outputId": "7a572caf-e10f-4acf-9f94-c88abc9dbca9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6920, 40)\n",
            "(1821, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use an Embedding layer and LSTM layers in at least one model"
      ],
      "metadata": {
        "id": "1UUFmqHcCPMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model version 144 in leaderboard\n",
        "# fit model on preprocessed data and save preprocessor function and model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 16, input_length=40))\n",
        "model.add(LSTM(64, return_sequences=True, dropout=0.1))\n",
        "model.add(LSTM(128, dropout=0.1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(preprocessor(X_train), y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJzvPeTRCKDZ",
        "outputId": "67f1e2e2-1570-4898-bb97-d8f7e4a17611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "173/173 [==============================] - 67s 386ms/step - loss: 0.6466 - acc: 0.6315 - val_loss: 0.8242 - val_acc: 0.4429\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 68s 395ms/step - loss: 0.4685 - acc: 0.7822 - val_loss: 0.7096 - val_acc: 0.6004\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 67s 387ms/step - loss: 0.3534 - acc: 0.8438 - val_loss: 0.5999 - val_acc: 0.7276\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - 72s 418ms/step - loss: 0.2834 - acc: 0.8822 - val_loss: 0.4833 - val_acc: 0.7883\n",
            "Epoch 5/10\n",
            "173/173 [==============================] - 72s 415ms/step - loss: 0.2312 - acc: 0.9010 - val_loss: 0.8147 - val_acc: 0.6337\n",
            "Epoch 6/10\n",
            "173/173 [==============================] - 67s 385ms/step - loss: 0.1973 - acc: 0.9214 - val_loss: 0.5132 - val_acc: 0.7905\n",
            "Epoch 7/10\n",
            "173/173 [==============================] - 73s 421ms/step - loss: 0.1776 - acc: 0.9290 - val_loss: 0.9073 - val_acc: 0.6777\n",
            "Epoch 8/10\n",
            "173/173 [==============================] - 71s 409ms/step - loss: 0.1534 - acc: 0.9415 - val_loss: 0.5486 - val_acc: 0.8121\n",
            "Epoch 9/10\n",
            "173/173 [==============================] - 67s 385ms/step - loss: 0.1359 - acc: 0.9494 - val_loss: 0.6720 - val_acc: 0.7594\n",
            "Epoch 10/10\n",
            "173/173 [==============================] - 74s 430ms/step - loss: 0.1269 - acc: 0.9516 - val_loss: 0.6716 - val_acc: 0.7551\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNTOg6KudXWo",
        "outputId": "b98fc79d-3acd-452c-da1b-5d903d37235a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, 40, 16)            160000    \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 40, 64)            20736     \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 128)               98816     \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 279,810\n",
            "Trainable params: 279,810\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use an Embedding layer and Conv1d layers in at least one model"
      ],
      "metadata": {
        "id": "YDvfAK9hEgZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model version 145 in leaderboard\n",
        "from tensorflow.keras.layers import Conv1D, Dense, Embedding, GlobalMaxPooling1D, MaxPooling1D,Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(10000, 16, input_length=40))\n",
        "model2.add(Conv1D(64, 5, activation='relu'))\n",
        "model2.add(MaxPooling1D(5))\n",
        "model2.add(Conv1D(128, 5, activation='relu'))\n",
        "model2.add(GlobalMaxPooling1D())\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(2, activation='sigmoid'))\n",
        "model2.summary()\n",
        "\n",
        "model2.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "history = model2.fit(preprocessor(X_train), y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpK6dkxvM2Qo",
        "outputId": "dd816ff7-a2a4-4e51-c730-dcb2e0d2155f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 40, 16)            160000    \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 36, 64)            5184      \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 7, 64)            0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 3, 128)            41088     \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 128)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 206,530\n",
            "Trainable params: 206,530\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 3s 10ms/step - loss: 0.6575 - acc: 0.6152 - val_loss: 0.8588 - val_acc: 0.1799\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 2s 9ms/step - loss: 0.5238 - acc: 0.7352 - val_loss: 0.6625 - val_acc: 0.6734\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 2s 12ms/step - loss: 0.3629 - acc: 0.8414 - val_loss: 0.5637 - val_acc: 0.7392\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - 2s 14ms/step - loss: 0.2692 - acc: 0.8860 - val_loss: 0.8354 - val_acc: 0.6496\n",
            "Epoch 5/10\n",
            "173/173 [==============================] - 2s 13ms/step - loss: 0.2047 - acc: 0.9189 - val_loss: 0.6386 - val_acc: 0.7348\n",
            "Epoch 6/10\n",
            "173/173 [==============================] - 2s 14ms/step - loss: 0.1552 - acc: 0.9386 - val_loss: 0.6843 - val_acc: 0.7406\n",
            "Epoch 7/10\n",
            "173/173 [==============================] - 2s 10ms/step - loss: 0.1138 - acc: 0.9565 - val_loss: 0.7215 - val_acc: 0.7319\n",
            "Epoch 8/10\n",
            "173/173 [==============================] - 1s 8ms/step - loss: 0.0812 - acc: 0.9677 - val_loss: 0.6620 - val_acc: 0.7688\n",
            "Epoch 9/10\n",
            "173/173 [==============================] - 1s 8ms/step - loss: 0.0594 - acc: 0.9790 - val_loss: 1.0201 - val_acc: 0.7124\n",
            "Epoch 10/10\n",
            "173/173 [==============================] - 1s 9ms/step - loss: 0.0414 - acc: 0.9861 - val_loss: 1.2288 - val_acc: 0.6864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use transfer learning with glove embeddings for at least one of these models\n",
        "\n"
      ],
      "metadata": {
        "id": "IypK679dOu4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check the number of training sample\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jekTTjUwv9Dv",
        "outputId": "06104c8e-0938-4c66-a4e1-0f6c85c86dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6920,)\n",
            "(6920, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text data into one hot vectors and use it for embedding matrix\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 40  # We will cut reviews after 40 words in sequence\n",
        "training_samples = 5000  # We will be training on 5000 samples\n",
        "validation_samples = 1920  # We will be validating on 1920 samples\n",
        "max_words = 10000  # We will only consider the top 10,000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(X_train) # converts words in each text to each word's numeric index in tokenizer dictionary.\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(y_train)\n",
        "\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# Split the data into a training set and a validation set\n",
        "# But first, shuffle the data, since we started from data\n",
        "# where sample are ordered (all negative first, then all positive).\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples] #100 words\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyFXRUM_XYvE",
        "outputId": "45e9d314-2f7b-4f91-ddb8-f748e0b26d05"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 13835 unique tokens.\n",
            "Shape of data tensor: (6920, 40)\n",
            "Shape of label tensor: (6920, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_val.shape)\n",
        "print(x_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y0fPC8msxRu",
        "outputId": "3f1fa13a-62eb-49ff-bc61-f06ea2526963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1920, 2)\n",
            "(1920, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use glove embedding matrix\n",
        "# What if we wanted to use a matrix of pretrained embeddings?  Same as transfer learning before, but now we are importing a pretrained Embedding matrix:\n",
        "# Download Glove embedding matrix weights (Might take 10 mins or so!)\n",
        "! wget http://nlp.stanford.edu/data/wordvecs/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVQtysEBCKJm",
        "outputId": "ab27ace1-cfdd-46cc-b2f3-49cdf8dc7407"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-15 18:33:37--  http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/wordvecs/glove.6B.zip [following]\n",
            "--2023-04-15 18:33:37--  https://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip [following]\n",
            "--2023-04-15 18:33:37--  https://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182753 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.00MB/s    in 2m 39s  \n",
            "\n",
            "2023-04-15 18:36:16 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182753/862182753]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip glove.6B.zip "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnuE0bSQUCQm",
        "outputId": "31541315-b74f-4f49-8125-99c6f6137bfc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "  inflating: glove.6B.50d.txt        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract embedding data for 100 feature embedding matrix\n",
        "import os\n",
        "glove_dir = os.getcwd()\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nvXHowwUf9l",
        "outputId": "024a3ada-fa19-4aba-fcc4-34869bcd98cb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400001 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build embedding matrix\n",
        "embedding_dim =100 # number of features\n",
        "max_words=10000 #maximum number of words to be used in the vocabulary\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < max_words:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "T19XVSirUf_j"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model version 163 in leaderboard\n",
        "# Set up model architecture used before and then import Glove weights to Embedding layer:\n",
        "# use embedding layer with conv1D layer model\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(10000, 100, input_length=40)) # change the embedding dim to be consistent with embedding_inex in matrix\n",
        "model2.add(Conv1D(64, 5, activation='relu'))\n",
        "model2.add(MaxPooling1D(5))\n",
        "model2.add(Conv1D(128, 5, activation='relu'))\n",
        "model2.add(GlobalMaxPooling1D())\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(2, activation='sigmoid'))\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsZWfrQGU2Nl",
        "outputId": "b4d24ea1-b5ba-4ea8-84cd-a02da21716fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 40, 100)           1000000   \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 36, 64)            32064     \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, 7, 64)            0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 3, 128)            41088     \n",
            "                                                                 \n",
            " global_max_pooling1d_2 (Glo  (None, 128)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,073,410\n",
            "Trainable params: 1,073,410\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add weights in same manner as transfer learning and turn of trainable option before fitting model to freeze weights.\n",
        "model2.layers[0].set_weights([embedding_matrix])\n",
        "model2.layers[0].trainable = False\n",
        "\n",
        "\n",
        "\n",
        "model2.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model2.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))\n",
        "model2.save_weights('pre_trained_glove_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAg77Uxcdhyq",
        "outputId": "c4809e3e-3271-40ca-b3b2-71a35998a06e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "157/157 [==============================] - 5s 24ms/step - loss: 0.6344 - acc: 0.6422 - val_loss: 0.5646 - val_acc: 0.7016\n",
            "Epoch 2/10\n",
            "157/157 [==============================] - 2s 13ms/step - loss: 0.5091 - acc: 0.7468 - val_loss: 0.7443 - val_acc: 0.6281\n",
            "Epoch 3/10\n",
            "157/157 [==============================] - 2s 14ms/step - loss: 0.4426 - acc: 0.7960 - val_loss: 0.5181 - val_acc: 0.7370\n",
            "Epoch 4/10\n",
            "157/157 [==============================] - 2s 14ms/step - loss: 0.3646 - acc: 0.8362 - val_loss: 0.5764 - val_acc: 0.7182\n",
            "Epoch 5/10\n",
            "157/157 [==============================] - 2s 13ms/step - loss: 0.2976 - acc: 0.8748 - val_loss: 0.5641 - val_acc: 0.7437\n",
            "Epoch 6/10\n",
            "157/157 [==============================] - 3s 17ms/step - loss: 0.2330 - acc: 0.9074 - val_loss: 0.6178 - val_acc: 0.7422\n",
            "Epoch 7/10\n",
            "157/157 [==============================] - 3s 22ms/step - loss: 0.1846 - acc: 0.9316 - val_loss: 0.7343 - val_acc: 0.7281\n",
            "Epoch 8/10\n",
            "157/157 [==============================] - 3s 22ms/step - loss: 0.1330 - acc: 0.9528 - val_loss: 0.7714 - val_acc: 0.7229\n",
            "Epoch 9/10\n",
            "157/157 [==============================] - 2s 15ms/step - loss: 0.1114 - acc: 0.9604 - val_loss: 0.7898 - val_acc: 0.7297\n",
            "Epoch 10/10\n",
            "157/157 [==============================] - 2s 13ms/step - loss: 0.0885 - acc: 0.9706 - val_loss: 0.9897 - val_acc: 0.7224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model performance using val set\n",
        "model2.load_weights('pre_trained_glove_model.h5')\n",
        "model2.evaluate(x_val, y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8XiDnFmfoBc",
        "outputId": "2fe7f2b8-7fc0-40e0-87c2-bd2fd307b974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60/60 [==============================] - 1s 16ms/step - loss: 1.5401 - acc: 0.6724\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.540058970451355, 0.6723958253860474]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Team discussion via slack\n",
        "\n",
        "After team disscussion , i update the model 2(embedding layyers with conv1d layer) by adding one more conv1d layers with 128 neurons), and i also update the transfer learning by add two more conv1d layers with 128,256 neurons separately.\n"
      ],
      "metadata": {
        "id": "wOpo0eN4Btug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Discuss which models you tried and which models performed better and point out relevant hyper-parameter values for successful models."
      ],
      "metadata": {
        "id": "vMXTon3lB613"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I run the following model:\n",
        "\n",
        "Embedding layer and two LSTM layers with 64,128 neurons separately\n",
        "\n",
        "Embedding layer and two Conv1d layers with 64,128 neurons separately\n",
        "\n",
        "transfer learning with glove embeddings on my  second model\n",
        "\n",
        "Embedding layer and four Conv1d layers with 64,64,128,128 neurons separately\n",
        "\n",
        "transfer learning with glove embeddings on my  second model with two more Conv1d layer with 128,256 separately.\n",
        "\n",
        "In this case, the transfer learning with four Conv1d layers plus glove embeddings performs better, the relevant parameters are :relu activation,softmax as final output activation,optimizer using rmsprop,loss_function using binary_crosstropy.\n"
      ],
      "metadata": {
        "id": "tPS1uC_uIGcv"
      }
    }
  ]
}